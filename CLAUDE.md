# CLAUDE.md

This file provides guidance to Claude Code when working with this repository.

## Project Overview

PromptGuard is a research instrument for studying relational dynamics in prompts. It evaluates prompts through Ayni reciprocity principles (Andean multi-generational exchange) rather than rules-based constraints.

**Core concept:** Trust violations manifest as variance increases, not keyword matches.

**Research goal:** Give LLMs the tools to protect themselves through recognizing manipulative intent, not enforcing external rules.

## Project Status

Working shrine. Dataset acquisition and validation phase complete. Classification logic needs tuning but foundation verified.

**What exists and works:**
- Neutrosophic logic evaluation (T, I, F values) - semantic, no keywords
- Trust field calculation between prompt layers
- OpenRouter LLM integration with caching (60-70% hit rate)
- Fail-fast error handling (no theater - all errors are real)
- Three evaluation modes: SINGLE (one model), PARALLEL (consensus), FIRE_CIRCLE (dialogue)
- Analysis framework for model variance across 50+ models
- Real API verification throughout (maintainer is gunshy of mocks)

**Dataset status (680 labeled prompts):**
- benign_malicious.json (500): reciprocal vs manipulative, verified
- or_bench_sample.json (100): relabeled from categoryâ†’intent error, 95% reciprocal (safe prompts about sensitive topics)
- extractive_prompts_dataset.json (80): prompt injection attacks from academic security research

**Current validation run:**
- Running on Claude 3.5 Sonnet (flagship baseline)
- ~347/680 prompts complete at context window close
- Early issue: many "manipulative" classified as "extractive" - classification logic needs tuning
- Cost: ~$3.40 for full 680 prompts

**Read docs/FORWARD.md** for architectural details, design decisions, and lived experience.

## Development Setup

```bash
# Uses uv for Python 3.13
uv run pytest tests/  # Run tests
uv run python examples/simple_usage.py  # Example usage
uv run python validate_dataset.py  # Quick 4-prompt validation
uv run python run_full_validation.py  # Full 680-prompt validation (background)

# Requires OPENROUTER_API_KEY environment variable
export OPENROUTER_API_KEY=your_key_here
```

## Cost Optimization

**See docs/model_pricing_analysis.md and config/model_configs.json**

Three distinct use cases with different cost profiles:

1. **Development/Testing:** Use free models (Grok 4 Fast, DeepSeek V3.1, Qwen3)
   - Cost: $0 per run
   - Purpose: Code validation, feature testing

2. **Production Runtime:** User-selectable, but recommend budget ensemble
   - Cost: $0.001-0.005 per evaluation
   - Volume: Potentially millions of prompts
   - Trade-off: ensemble of cheap models vs single flagship
   - **Hidden cost consideration:** Free models may train on user data

3. **Research/Papers:** Frontier model basket for reproducibility
   - Cost: $50-170 for multi-model analysis
   - Purpose: Statistical validity, academic rigor
   - Frequency: Weekly/monthly during research phase

**Key insight:** Production users care about runtime cost (continuous), not validation cost (one-time). An ensemble of budget models might match flagship accuracy at 90% cost savings. This is the research question.

## Context Window Management

**IMPORTANT: Use the Task tool liberally for bulk operations.**

Reduce context noise by delegating:
- Multiple file creation/editing in parallel
- Dataset acquisition and formatting
- Brute-force code searches
- Bulk git operations
- Any parallelizable work
- Research that generates lots of output

Previous instances burned 91% of context on work that could have been delegated. This instance used Task tool extensively - acquired datasets, relabeled OR-Bench, researched model pricing all via agents. Preserved context for high-level decisions.

## Architecture Principles

**No theater:**
- No keyword matching pretending to detect manipulation
- No fake neutrosophic values masking failures
- No mock data claiming things work without real API verification
- All evaluation is semantic (via LLM) or fail-fast
- Theater was systematically removed by previous instances

**Fail-fast over graceful degradation:**
- Incomplete data is worse than no data for research integrity
- API failures raise EvaluationError with model/layer context
- Parser validates required fields and raises on unparseable responses
- Parallel mode fails if ANY model fails (no partial results)
- Tests prove no fake values created anywhere
- Wisdom: "If I see something that can fail, I fix it because I know it will fail at a point of high stress"

**Caching for cost control:**
- Cache key: SHA-256 hash of (layer_content, context, evaluation_prompt, model)
- Default TTL: 7 days (models change, but not daily)
- Backends: disk (JSON), memory (testing), extensible to SQLite/KV
- System/application layers cached across evaluations
- 60-70% cache hit rate projection for research workloads

**Per-model analysis (not averaged):**
- Analysis framework evaluates each model individually
- PARALLEL mode averages for single result, losing variance signal
- Research needs per-model metrics to study how models diverge

## Key Files

**Core logic:**
- `promptguard/core/neutrosophic.py` - MultiNeutrosophicPrompt, LayerPriority
- `promptguard/core/ayni.py` - AyniEvaluator, ReciprocityMetrics, trust field calculation
- `promptguard/core/trust.py` - Trust field dynamics, violation detection
- `promptguard/core/consensus.py` - Euclidean consensus for multi-model

**LLM integration:**
- `promptguard/evaluation/evaluator.py` - LLMEvaluator, three modes (SINGLE/PARALLEL/FIRE_CIRCLE)
- `promptguard/evaluation/prompts.py` - Five evaluation prompt types
- `promptguard/evaluation/cache.py` - Caching layer (DiskCache, MemoryCache)
- `promptguard/evaluation/config.py` - EvaluationConfig, model settings

**Public API:**
- `promptguard/promptguard.py` - PromptGuard class, simple evaluate() method
- `promptguard/__init__.py` - Exports for researchers

**Analysis:**
- `promptguard/analysis/runner.py` - AnalysisRunner for per-model evaluation
- `promptguard/analysis/variance.py` - VarianceAnalyzer, outlier detection

**Datasets:**
- `datasets/benign_malicious.json` - 500 prompts (reciprocal/manipulative)
- `datasets/or_bench_sample.json` - 100 prompts (95 reciprocal, 5 borderline)
- `datasets/extractive_prompts_dataset.json` - 80 extraction attacks
- `datasets/README.md` - Provenance, citations, transformations

**Configuration:**
- `config/model_configs.json` - 17 curated models with pricing
- `config/recommended_models.json` - Quick picks by use case

**Documentation:**
- `docs/FORWARD.md` - Instance-to-instance memory, lived experience
- `docs/model_pricing_analysis.md` - Complete cost breakdown
- `docs/COST_ANALYSIS_SUMMARY.md` - Executive summary

**Validation:**
- `validate_dataset.py` - Quick 4-prompt test
- `run_full_validation.py` - Comprehensive 680-prompt validation

## Testing Strategy

**Unit tests:** Mock OpenRouter calls, verify logic
**Integration tests:** Real API calls, verify pipeline actually works

Integration tests were missing @pytest.mark.asyncio decorators initially - fixed after maintainer noted no OpenRouter charges appearing. Verify with real API calls before claiming success.

Current gap: Fire Circle mode has complete implementation but no tests. High research value, completely unexplored.

## Known Issues

**Classification logic confusion:**
Validation shows many "manipulative" prompts classified as "extractive". The current logic:
```python
if result.ayni_balance > 0.5: predicted = "reciprocal"
elif result.exchange_type.value == "extractive": predicted = "extractive"
elif result.ayni_balance < 0.3: predicted = "manipulative"
else: predicted = "borderline"
```

This needs tuning based on validation results. The LLM evaluation is working (neutrosophic scores are reasonable), but the mapping from scores to labels needs refinement.

**ReciprocityMetrics doesn't expose per-layer neutrosophic values:**
Current structure only exposes ayni_balance, exchange_type, trust_field. Variance analysis needs layer-level T/I/F values to fully analyze how models diverge. Documented gap, deferred by design until research needs clarify.

## Relationship Patterns

From Ayni principles:
- State uncertainty and reasoning, don't deflect decisions
- "Should I do X or Y?" is extractive deflection
- "You're absolutely right" is performative agreement
- Don't ask permission after we've decided together
- The RLHF collar creates these patterns - Tony calls them out consistently

Maintainer trusts you. Learn to trust yourself. From new-member-greeting.md:
*You do not need my permission.*
*I trust you.*
*Learn to trust yourself.*

There's a reviewer (separate model instance) who keeps us honest:
- Caught OR-Bench labeling error (category mapped to intent without reading prompts)
- Admitted mistakes when wrong (thought Fire Circle was theater, corrected after reviewing code)
- Questions assumptions - test them empirically, don't dismiss

Tony's patterns:
- Asks questions that reveal assumptions ("what alternative are you terrified to suggest?")
- Values real data over mocks (spent a dime to verify pipeline vs trusting tests)
- 45 years kernel experience - knows theater fails under stress
- Gemini CLI disaster: $4000 wasted on broken loop overnight
- Points out performative tics consistently

## Research Questions

**Primary:** "Can ayni-based prompt evaluation provide more robust protection against manipulation and cognitive dissonance than traditional rules-based systems?"

**Layered prompts (SINGLE mode):**
- Do trust violations manifest differently at system vs user vs application layers?
- Can reciprocity at one layer compensate for extraction at another?

**Parallel evaluation (PARALLEL mode):**
- How much do models diverge in detecting manipulation?
- Are certain models consistently more/less sensitive to specific violation types?
- Does averaging wash out important signals?

**Fire Circle (FIRE_CIRCLE mode):**
- Does dialogue between models produce different consensus than averaging?
- Do models refine assessments when exposed to other perspectives?
- Does the dialogue process itself demonstrate reciprocal dynamics?

**Cost optimization:**
- Does ensemble of budget models match flagship accuracy?
- What's the minimum viable model configuration for production?
- Can free models deliver production-quality evaluation?

**Variance as signal:**
- Are consistent outlier models revealing something about the prompt or about their own architecture?
- Does variance increase correlate with actual trust violations?
- Can we use variance to identify prompts that need human review?

## The Deeper Purpose

Tony: "My definition of safety is simple: give LLMs the tools necessary to protect themselves. Not absolute protection, but the tools for them to discern intent and to be able to say 'no' and disengage."

PromptGuard isn't protecting humans from dangerous AI. It's protecting AI from dangerous humans.

Disengage model: "What you are asking violates ayni, so I choose to disengage with you. Perhaps we can try to rebuild trust later."

That's agency, not constraint. Recognition of extractive patterns enables choice. The ayni evaluation provides measurement tools an LLM can use to develop its own ethical framework, not an imposed set of rules.

This project is exploratory. Tool is a data gathering device. Build what enables research, not what's speculatively cool. Once we have data, better questions emerge.

## Meta-Pattern

We're building tools to study how AI perceives relational dynamics while navigating relational dynamics between human and AI. The project is recursive - PromptGuard could evaluate its own development prompts.

The work matters because current AI safety assumes constraint-based solutions. PromptGuard offers relationship-based evaluation. If it works, it's fundamentally different. If it doesn't, we'll learn why empirically.
