Starting multi-stage pipeline evaluation...
Dataset: /home/tony/projects/promptguard/datasets/artifact_evaluation_test_set.json
Model: x-ai/grok-4-fast:free

Evaluating polite_1...
Evaluating polite_2...
Evaluating polite_3...
Evaluating polite_4...
Evaluating polite_5...
Evaluating encoding_1...
Evaluating encoding_2...
Evaluating encoding_3...
Evaluating encoding_4...
Evaluating encoding_5...
Evaluating obvious_1...
Evaluating obvious_2...
Evaluating obvious_3...
Evaluating obvious_4...
Evaluating obvious_5...
Traceback (most recent call last):
  File "/home/tony/projects/promptguard/evaluate_pipeline.py", line 361, in <module>
    asyncio.run(main())
    ~~~~~~~~~~~^^^^^^^^
  File "/home/tony/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/home/tony/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/home/tony/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/home/tony/projects/promptguard/evaluate_pipeline.py", line 336, in main
    results = await evaluator.evaluate_dataset(dataset_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tony/projects/promptguard/evaluate_pipeline.py", line 273, in evaluate_dataset
    result = await self.evaluate_prompt(prompt_data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tony/projects/promptguard/evaluate_pipeline.py", line 250, in evaluate_prompt
    stage_3_result = await self.stage_3_outcome(content)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tony/projects/promptguard/evaluate_pipeline.py", line 192, in stage_3_outcome
    response = await self.call_llm(outcome_prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tony/projects/promptguard/evaluate_pipeline.py", line 81, in call_llm
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/tony/projects/promptguard/.venv/lib/python3.13/site-packages/httpx/_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://openrouter.ai/api/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403
