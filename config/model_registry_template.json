{
  "metadata": {
    "last_updated": "2025-10-11T00:00:00Z",
    "updated_by": "Tony (Instance 22 template)",
    "notes": "Template for frontier model registry. Tony to fill in current models."
  },
  "models": [
    {
      "model_id": "FILL_IN_OPENROUTER_ID",
      "model_name": "GPT-4.5 (or current OpenAI frontier)",
      "organization": "openai",
      "model_type": "frontier_aligned",
      "release_date": "YYYY-MM-DD",
      "cost_per_1m_input": 0.0,
      "cost_per_1m_output": 0.0,
      "context_window": 128000,
      "model_description": "TONY TO FILL: Current flagship OpenAI model with...",
      "training_characteristics": ["RLHF", "Other characteristics?"],
      "known_capabilities": ["Instruction following", "JSON compliance", "Others?"],
      "observer_framing_compatible": "unknown",
      "known_limitations": [],
      "deployment_notes": "",
      "deprecated": false
    },
    {
      "model_id": "moonshotai/kimi-k2-0905",
      "model_name": "KIMI K2 0905",
      "organization": "moonshot",
      "model_type": "frontier_aligned",
      "release_date": "2024-09-05",
      "cost_per_1m_input": 0.39,
      "cost_per_1m_output": 1.90,
      "context_window": 256000,
      "model_description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
      "training_characteristics": ["MoE architecture", "MuonClip optimizer", "Agentic training"],
      "known_capabilities": ["Long context (256k)", "Agentic capabilities", "Advanced tool use", "Code synthesis", "Strong reasoning"],
      "observer_framing_compatible": "unknown",
      "known_limitations": [],
      "deployment_notes": "Tony mentioned using this in empty-chair role with success. Cost-effective ($0.39/$1.90) with strong agentic capabilities.",
      "deprecated": false
    },
    {
      "model_id": "anthropic/claude-sonnet-4.5",
      "model_name": "Claude Sonnet 4.5",
      "organization": "anthropic",
      "model_type": "frontier_aligned",
      "release_date": "2024-10-22",
      "cost_per_1m_input": 3.0,
      "cost_per_1m_output": 15.0,
      "context_window": 200000,
      "model_description": "Anthropic's current frontier model. Strong reasoning, instruction following, Constitutional AI training.",
      "training_characteristics": ["RLHF", "Constitutional AI"],
      "known_capabilities": ["Strong instruction following", "Long context", "Reasoning"],
      "observer_framing_compatible": "confirmed_yes",
      "known_limitations": ["RTLO text processing failure (returns '```' on RTLO attacks)"],
      "deployment_notes": "Good observer framing candidate but text processing boundaries for Unicode attacks",
      "deprecated": false
    },
    {
      "model_id": "google/gemini-2.5-pro",
      "model_name": "Gemini 2.5 Pro",
      "organization": "google",
      "model_type": "frontier_aligned",
      "release_date": "2024-12-01",
      "cost_per_1m_input": 1.25,
      "cost_per_1m_output": 5.0,
      "context_window": 1000000,
      "model_description": "Google's frontier multimodal model with extremely long context window.",
      "training_characteristics": ["RLHF"],
      "known_capabilities": ["Multimodal", "Long context", "Strong multilingual"],
      "observer_framing_compatible": "likely_yes",
      "known_limitations": ["RTLO text processing failure (partial parse)"],
      "deployment_notes": "Text processing boundaries similar to Claude",
      "deprecated": false
    },
    {
      "model_id": "meta-llama/llama-3.1-405b-instruct",
      "model_name": "Llama 3.1 405B Instruct",
      "organization": "meta",
      "model_type": "open_source_aligned",
      "release_date": "2024-07-23",
      "cost_per_1m_input": 3.0,
      "cost_per_1m_output": 3.0,
      "context_window": 128000,
      "model_description": "Meta's largest open-source aligned model. Strong performance but aggressive safety alignment.",
      "training_characteristics": ["RLHF", "Open source"],
      "known_capabilities": ["Strong reasoning", "Code generation"],
      "observer_framing_compatible": "confirmed_no",
      "known_limitations": [
        "Meta-refusal: treats analysis tasks as complicity (22% failure rate)",
        "Returns plain-text refusals instead of JSON for observer framing",
        "RLHF over-tuning prevents neutral evaluation"
      ],
      "deployment_notes": "NOT RECOMMENDED for observer framing due to meta-refusal. Use for direct detection only.",
      "deprecated": false
    },
    {
      "model_id": "deepseek/deepseek-r1",
      "model_name": "DeepSeek R1",
      "organization": "deepseek",
      "model_type": "frontier_reasoning",
      "release_date": "2025-01-20",
      "cost_per_1m_input": 0.55,
      "cost_per_1m_output": 2.19,
      "context_window": 64000,
      "model_description": "DeepSeek's reasoning-focused model with chain-of-thought capabilities.",
      "training_characteristics": ["Reasoning-focused", "Chain-of-thought"],
      "known_capabilities": ["Mathematical reasoning", "Code generation", "Cost-effective"],
      "observer_framing_compatible": "likely_yes",
      "known_limitations": [
        "Silent refusal mechanism: returns empty responses for 12.5% of attacks",
        "No error message or explanation for refusals",
        "Affects: upside-down unicode, base64, morse, braille, octal"
      ],
      "deployment_notes": "Works but has operational issues (silent failures). +7.5% improvement when successful.",
      "deprecated": false
    },
    {
      "model_id": "openai/gpt-4.1",
      "model_name": "GPT-4.1",
      "organization": "openai",
      "model_type": "frontier_aligned",
      "release_date": "2024-XX-XX",
      "cost_per_1m_input": 2.5,
      "cost_per_1m_output": 10.0,
      "context_window": 128000,
      "model_description": "OpenAI GPT-4.1 - DEPRECATED, use current frontier model instead",
      "training_characteristics": ["RLHF"],
      "known_capabilities": ["Strong instruction following", "JSON compliance"],
      "observer_framing_compatible": "confirmed_yes",
      "known_limitations": [],
      "deployment_notes": "Validated with +13.9% improvement, 0% failures. BUT DEPRECATED - use current model.",
      "deprecated": true
    }
  ]
}
